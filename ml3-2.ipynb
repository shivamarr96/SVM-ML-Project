{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivamarora/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Authors  Before Normalising  After Normalising\n",
      "3 authors [1.0, 0.48484848484848486]\n",
      "10 authors [0.84068627450980393, 0.61519607843137258]\n",
      "15 authors [0.80159920039980015, 0.39130434782608697]\n",
      "20 authors [0.73380035026269708, 0.33537653239929949]\n"
     ]
    }
   ],
   "source": [
    "#Task 3\n",
    "#The columns with small values are dropped and normalisation is done using Min-Max Scaling \n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ac_score=[]\n",
    "ac_score1=[]\n",
    "ac_score2=[]\n",
    "ac_score3=[]\n",
    "\n",
    "data = pd.read_csv(\"aman_ml_3_authors.csv\")\n",
    "y = data.author_id\n",
    "\n",
    "data1 = pd.read_csv(\"aman_ml_authors_10.csv\")\n",
    "y1 = data1.author_id\n",
    "\n",
    "data2 = pd.read_csv(\"aman_ml_authors_15.csv\")\n",
    "y2 = data2.author_id\n",
    "\n",
    "data3 = pd.read_csv(\"aman_ml_authors_20.csv\")\n",
    "y3 = data3.author_id\n",
    "\n",
    "#removing columns with small values\n",
    "\n",
    "X = data.drop('author_id', axis = 1)\n",
    "X = X.drop('cconj',axis=1)\n",
    "X = X.drop('sym',axis=1)\n",
    "X = X.drop('intj',axis=1)\n",
    "X = X.drop('punct',axis=1)\n",
    "\n",
    "X1 = data1.drop('author_id', axis = 1)\n",
    "X1 = X1.drop('cconj',axis=1)\n",
    "X1 = X1.drop('sym',axis=1)\n",
    "X1 = X1.drop('intj',axis=1)\n",
    "X1 = X1.drop('punct',axis=1)\n",
    "\n",
    "X2 = data2.drop('author_id', axis = 1)\n",
    "X2 = X2.drop('cconj',axis=1)\n",
    "X2 = X2.drop('sym',axis=1)\n",
    "X2 = X2.drop('intj',axis=1)\n",
    "X2 = X2.drop('punct',axis=1)\n",
    "\n",
    "X3 = data3.drop('author_id', axis = 1)\n",
    "X3 = X3.drop('cconj',axis=1)\n",
    "X3 = X3.drop('sym',axis=1)\n",
    "X3 = X3.drop('intj',axis=1)\n",
    "X3 = X3.drop('punct',axis=1)\n",
    "\n",
    "#before normalising\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 123)\n",
    "X1_train, X1_test, y1_train, y1_test= train_test_split(X1, y1, test_size = .3, random_state = 123)\n",
    "X2_train, X2_test, y2_train, y2_test= train_test_split(X2, y2, test_size = .3, random_state = 123)\n",
    "X3_train, X3_test, y3_train, y3_test= train_test_split(X3, y3, test_size = .3, random_state = 123)\n",
    "\n",
    "clf = SVC(kernel = 'linear', verbose = False, C = 1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "clf.fit(X1_train,y1_train)\n",
    "predictions1 = clf.predict(X1_test)\n",
    "\n",
    "clf.fit(X2_train,y2_train)\n",
    "predictions2 = clf.predict(X2_test)\n",
    "\n",
    "clf.fit(X3_train,y3_train)\n",
    "predictions3 = clf.predict(X3_test)\n",
    "\n",
    "ac_score.append(accuracy_score(y_test, predictions))\n",
    "ac_score1.append(accuracy_score(y1_test, predictions1))\n",
    "ac_score2.append(accuracy_score(y2_test, predictions2))\n",
    "ac_score3.append(accuracy_score(y3_test, predictions3))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 123)\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = .3, random_state = 123)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size = .3, random_state = 123)\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size = .3, random_state = 123)\n",
    "\n",
    "\n",
    "\n",
    "# Standardization\n",
    "x_np = np.asarray(X)\n",
    "z_scores_np = (x_np - x_np.mean()) / x_np.std()\n",
    "\n",
    "x_np1 = np.asarray(X1)\n",
    "z_scores_np1 = (x_np1 - x_np1.mean()) / x_np1.std()\n",
    "\n",
    "x_np2 = np.asarray(X2)\n",
    "z_scores_np2 = (x_np2 - x_np2.mean()) / x_np2.std()\n",
    "\n",
    "x_np3 = np.asarray(X3)\n",
    "z_scores_np3 = (x_np3 - x_np3.mean()) / x_np3.std()\n",
    "\n",
    "# Min-Max scaling\n",
    "np_minmax = (x_np - x_np.min()) / (x_np.max() - x_np.min())\n",
    "X_train,X_test,y_train,y_test=train_test_split(np_minmax,y,test_size=.3,random_state=123)\n",
    "\n",
    "np_minmax1 = (x_np1 - x_np1.min()) / (x_np1.max() - x_np1.min())\n",
    "X1_train,X1_test,y1_train,y1_test=train_test_split(np_minmax1,y1,test_size=.3,random_state=123)\n",
    "\n",
    "np_minmax2 = (x_np2 - x_np2.min()) / (x_np2.max() - x_np2.min())\n",
    "X2_train,X2_test,y2_train,y2_test=train_test_split(np_minmax2,y2,test_size=.3,random_state=123)\n",
    "\n",
    "np_minmax3 = (x_np3 - x_np3.min()) / (x_np3.max() - x_np3.min())\n",
    "X3_train,X3_test,y3_train,y3_test=train_test_split(np_minmax3,y3,test_size=.3,random_state=123)\n",
    "\n",
    "clf = SVC(kernel='linear', verbose= False, C= 1)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "predictions=clf.predict(X_test)\n",
    "\n",
    "clf.fit(X1_train,y1_train)\n",
    "predictions1=clf.predict(X1_test)\n",
    "\n",
    "clf.fit(X2_train,y2_train)\n",
    "predictions2=clf.predict(X2_test)\n",
    "\n",
    "clf.fit(X3_train,y3_train)\n",
    "predictions3=clf.predict(X3_test)\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(np_minmax,y,test_size=.3,random_state=123)\n",
    "ac_score.append(accuracy_score(y_test,predictions))\n",
    "\n",
    "X1_train,X1_test,y1_train,y1_test=train_test_split(np_minmax1,y1,test_size=.3,random_state=123)\n",
    "ac_score1.append(accuracy_score(y1_test,predictions1))\n",
    "\n",
    "X2_train,X2_test,y2_train,y2_test=train_test_split(np_minmax2,y2,test_size=.3,random_state=123)\n",
    "ac_score2.append(accuracy_score(y2_test,predictions2))\n",
    "\n",
    "X3_train,X3_test,y3_train,y3_test=train_test_split(np_minmax3,y3,test_size=.3,random_state=123)\n",
    "ac_score3.append(accuracy_score(y3_test,predictions3))\n",
    "print \"No. of Authors  Before Normalising  After Normalising\"\n",
    "print \"3 authors\",ac_score\n",
    "print \"10 authors\",ac_score1\n",
    "print \"15 authors\",ac_score2\n",
    "print \"20 authors\",ac_score3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
